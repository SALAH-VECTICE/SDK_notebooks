{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "German-Credit-Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    },
    "interpreter": {
      "hash": "5970002f2814f9b5c80d9d4fc10f2bf2fda215a6ce99a901ef026501edeb9abe"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SALAH-VECTICE/SDK_notebooks/blob/main/Notebooks/Vanilla/German_Credit_Analysis/German-Credit-Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24zCMu8i0GVD"
      },
      "source": [
        "!pip3 install -q fsspec\n",
        "!pip3 install -q gcsfs\n",
        "!pip3 install -q vectice\n",
        "!pip3 install -q mlflow\n",
        "!pip3 install -q google-cloud-storage\n",
        "!pip3 install -q chart_studio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "EBflSMN64iP-"
      },
      "source": [
        "!pip3 show vectice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "PzY8huTg4iQB"
      },
      "source": [
        "The main entrypoint of the SDK is the high level API which provide several solutions to follow your runs.\n",
        "\n",
        "* a procedural solution with 2 methods to call vectice.create_run() and vectice.save_after_run()\n",
        "\n",
        "* a more powerful solution based on vectice.Vectice class that provides itself several possibilities:\n",
        "\n",
        "* use an instance of vectice.Vectice object to create_run(), start_run() and end_run() (fluent API)\n",
        "\n",
        "* You can also use the context manager syntax (python with keyword): In this case, the end of the run will be automatically managed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "oNNRfqJ20Sdr",
        "outputId": "5bd7c065-6e13-42d5-e25f-43f584c7ea78"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from plotly import tools\n",
        "import chart_studio.plotly as py\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "import plotly.offline as pyo\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "from google.cloud import storage\n",
        "import mlflow\n",
        "from vectice import Vectice\n",
        "from vectice.models import JobType\n",
        "from vectice.entity.model import ModelType\n",
        "from vectice.entity.model_version import ModelVersionStatus\n",
        "\n",
        "pyo.init_notebook_mode()\n",
        "init_notebook_mode(connected=True)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8Q4_bSy5SEt"
      },
      "source": [
        "Here is a link to the Python SDK Documentation.\n",
        "[Python SDK Documentation](https://storage.googleapis.com/sdk-documentation/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OqZNpTq0pVw"
      },
      "source": [
        "### Goals for this Project\n",
        "* Explore our data and detecting key patterns.\n",
        "* Develop a Neural Network to predict whether a loan will be of a good or bad risk.\n",
        "* Most importantly, have fun while doing this project.\n",
        "### Brief Overview:\n",
        "The first phase of this project is to see what is our data made about. Which variables are numerical or categorical and which columns have \"Null\" values, which is something we will address in the feature engineering phase.\n",
        "\n",
        "#### Summary:\n",
        "* We have four numeric and four categorical features.\n",
        "* The average age of people in our dataset is 35.54\n",
        "* The average credit amount borrowed is 3271"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0BRycdPVlsw"
      },
      "source": [
        "Make sure you declare the GCS credentials as an environmental as seen below. Or any other way you prefer so we can access the files in GCS.\n",
        "\n",
        "```\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'FILE.json'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO9mBuZv0vvL"
      },
      "source": [
        "# In Google Collab you can load your json key file to access GCS that was provided with your tutorial account. \n",
        "# The name should be something like test.json.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKPmq3ck4iQS"
      },
      "source": [
        "## Vectice Credentials \n",
        "\n",
        "To connect to the Vectice App through the SDK you'll need the Project Token, Vectice API Endpoint and the Vectice API Token. You'll find all of this in the Vectice App. The Workspace allows you to create the Vectice API Token, in Projects you'll be able to get the Project Token, as seen below. The Vectice API Endpoint is 'https://be-beta.vectice.com'. You're provided with the GCS Service Account JSON, this will allow you to connect to the GCS Bucket in the Vectice App and get the needed data for the example. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ZriEQ5aJuR"
      },
      "source": [
        "## Credentials Setup:\n",
        "The Vectice API Endpoint and Token are needed to connect to the Vectice UI. Furthermore, a Google Cloud Storage credential JSON is needed to connect to the Google Cloud Storage to retrieve and upload the datasets. A project token links the runs to the relevant project and it's needed to create runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYmWxrF40l93"
      },
      "source": [
        "os.environ['VECTICE_API_ENDPOINT'] = 'https://beta.vectice.com'\n",
        "# Workspace -> API Tokens from Vectice App\n",
        "os.environ['VECTICE_API_TOKEN'] = \"CONNECTION_TOKEN\"\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"FILE.json\"\n",
        "# Project token from Vectice App\n",
        "PROJECT_TOKEN = \"PROJECT_TOKEN\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDUREMGc0012"
      },
      "source": [
        "# Create Vectice instance \n",
        "vectice = Vectice(project_token=PROJECT_TOKEN)\n",
        "# It will specify the dataset version we just created as the run's input. *NB - You need to add a dataset called \"German-Credit-Data\" in your Vectice Project.\n",
        "ds_version = [vectice.create_dataset_version().with_parent_name(\"German-Credit-Data\")]\n",
        "# Create a run \n",
        "run = vectice.create_run('Data Cleaning', JobType.PREPARATION)\n",
        "# Start a run to track this data cleaning job\n",
        "vectice.start_run(run, inputs = ds_version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rlbcYt44vf4"
      },
      "source": [
        "This is an example how you would push your data into your GCS bucket.\n",
        "\n",
        "```\n",
        "data.to_csv(\"gs://BUCKET/FILE_PATH/FILENAME.csv\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR3rb4tK37H6"
      },
      "source": [
        "The dataset used in this tutorial is retrieved from a Google Cloud Storage Bucket. Throughout the notebook you will notice that interacting with the Google Cloud Storage Bucket and it's relatively easy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDafSbUO08g4"
      },
      "source": [
        "# Get data from GCS Storage bucket\n",
        "df = pd.read_csv(\"gs://vectice-examples-samples/German_Credit/german_credit_data.csv\", index_col=0)\n",
        "# Create a copy of the dataframe to use later\n",
        "original_df = df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNr2BAyq1RAu"
      },
      "source": [
        "# Rename the column\n",
        "df = df.rename(columns={\"Credit amount\": \"Credit_amount\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP8odZs8H47_"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rajaLmJG4iQf"
      },
      "source": [
        "You could then ```df.to_csv(\"gs://BUCKET/FILE_PATH/FILENAME.csv\")``` to push the dataset to the GCS Bucket and then add the dataset to the Vectice App through the UI for this datatset that will be used in the Exploratory Data Analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy7FpSRv1VIR"
      },
      "source": [
        "# Create outputs\n",
        "outputs = [vectice.create_dataset_version().with_parent_name(\"EDA data\")]\n",
        "# End the run and save the new dataset version.\n",
        "# Set the orginal_cleaned as an output.\n",
        "vectice.end_run(outputs = outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVw40tiI1Z9F"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjzdKYzC1bu-"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnLzLB8T1dPm"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3SuLR651e4e"
      },
      "source": [
        "# Check missing values\n",
        "df.isnull().sum().sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6JPcbmz1gK-"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6ScX6lR1h4u"
      },
      "source": [
        "# Distribution of Credit_Amount for each Gender\n",
        "male_credit = df[\"Credit_amount\"].loc[df[\"Sex\"] == \"male\"].values\n",
        "female_credit = df[\"Credit_amount\"].loc[df[\"Sex\"] == \"female\"].values\n",
        "total_credit = df['Credit_amount'].values\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(16,4))\n",
        "\n",
        "sns.distplot(male_credit, ax=ax[0], color=\"#FE642E\")\n",
        "ax[0].set_title(\"Male Credit Distribution\", fontsize=16)\n",
        "sns.distplot(female_credit, ax=ax[1], color=\"#F781F3\")\n",
        "ax[1].set_title(\"Female Credit Distribution\", fontsize=16)\n",
        "sns.distplot(total_credit, ax=ax[2], color=\"#2E64FE\")\n",
        "ax[2].set_title(\"Total Credit Distribution\", fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmUDklut1kKO"
      },
      "source": [
        "plt.figure(figsize=(13,6)) #figure size\n",
        "g = sns.boxplot(x='Purpose', y='Credit_amount', \n",
        "                   data=df, palette=\"RdBu\")\n",
        "\n",
        "\n",
        "g.set_title(\"Credit Distribution by Purpose\", fontsize=16)\n",
        "g.set_xticklabels(g.get_xticklabels(),rotation=45) # It's the way to rotate the xticks when we use variable to our graphs\n",
        "g.set_xlabel('Device Names', fontsize=18) # Xlabel\n",
        "g.set_ylabel('Trans Revenue(log) Dist', fontsize=18) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6LSdmia1r8_"
      },
      "source": [
        "### Analysis by Group:\n",
        "#### Gender Analysis:\n",
        "In this section analyze the gender section of our dataset.\n",
        "\n",
        "#### Objectives:\n",
        "* Find the distribution of genders in our dataset.\n",
        "* See the distribution o each gender by the age (For instance, we have a higher number of young males than younger females)\n",
        "* What were the main application reasons for a credit loan? Does it vary by Gender?\n",
        "* How many jobs does each gender have? How many are Unemployed?\n",
        "\n",
        "#### Summary:\n",
        "* Theres 2x more males than females in our dataset.\n",
        "* Most females that applied for a credit loan were less than 30 .\n",
        "* Most of the males that applied for a loan ranged from their 20s-40s\n",
        "* Females were more likely to apply for a credit loan tobuy furniture and equipment. (10% more than males)\n",
        "* Males applied 2x more than females for a credit loan to invest in a business.\n",
        "* 2x of females were unemployed compared to males.\n",
        "* 2x of males worked 3 jobs compared to females.\n",
        "* Suprisingly, most people that applied for a credit loan have two jobs!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Img_hPZl1pd3"
      },
      "source": [
        "# We have 2x more German males applying for Credit Loans than Females.\n",
        "df[\"Sex\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpS8CP-T1v8n"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "by_age = df['Age'].values.tolist()\n",
        "male_age = df['Age'].loc[df['Sex'] == 'male'].values.tolist()\n",
        "female_age = df['Age'].loc[df['Sex'] == 'female'].values.tolist()\n",
        "\n",
        "trace0 = go.Histogram(\n",
        "    x=male_age,\n",
        "    histnorm='probability',\n",
        "    name=\"German Male\",\n",
        "    marker = dict(\n",
        "        color = 'rgba(100, 149, 237, 0.6)',\n",
        "    )\n",
        ")\n",
        "trace1 = go.Histogram(\n",
        "    x=female_age,\n",
        "    histnorm='probability',\n",
        "    name=\"German Female\",\n",
        "    marker = dict(\n",
        "        color = 'rgba(255, 182, 193, 0.6)',\n",
        "    )\n",
        ")\n",
        "trace2 = go.Histogram(\n",
        "    x=by_age,\n",
        "    histnorm='probability',\n",
        "    name=\"Overall Gender\",\n",
        "     marker = dict(\n",
        "        color = 'rgba(169, 169, 169, 0.6)',\n",
        "    )\n",
        ")\n",
        "fig = tools.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n",
        "                          subplot_titles=('Males','Female', 'All Genders'))\n",
        "\n",
        "fig.append_trace(trace0, 1, 1)\n",
        "fig.append_trace(trace1, 1, 2)\n",
        "fig.append_trace(trace2, 2, 1)\n",
        "\n",
        "fig['layout'].update(showlegend=True, title='Distribution of Gender', bargap=0.05)\n",
        "# iplot(fig, filename='custom-sized-subplot-with-subplot-titles')\n",
        "HTML(fig.to_html())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVKy0hiu1yVu"
      },
      "source": [
        "# Gender vs Purpose let's see the purpose of having credit loans for each gender.\n",
        "df[\"Purpose\"].unique()\n",
        "sex_purpose = pd.crosstab(df['Purpose'], df['Sex']).apply(lambda x: x/x.sum() * 100)\n",
        "sex_purpose"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKC9vVqA4VpD"
      },
      "source": [
        "### Age Groups:\n",
        "In this section we will create categorical groups based on the age column. The following categorical variables will belong to the \"Age_Group\" column:\n",
        "\n",
        "* Young: Clients age ranges from (19 - 29).\n",
        "* Young Adults: Clients age ranges from (30-40)\n",
        "* Senior: Clients age ranges from (41-55)\n",
        "* Elder: Clients age is more than 55 years old\n",
        " \n",
        "#### What we want to accomplish:\n",
        "* Create different age groups based on their age.\n",
        "* See the Credit amounts borrowed by clients belonging to each age group.\n",
        "* Get deeper in our analysis and determine which loans were high risk and see if there is any patterns with regards to age groups.\n",
        "\n",
        "#### Summary:\n",
        "* The younger age group tended to ask slightly for higher loans compared to the older age groups.\n",
        "* The young and elederly groups had the highest ratio of high risk loans. With 45.29% of all the clients that belong to the young age group being considered of high risk.\n",
        "* The number of loans that were considered of high risk within the elderly group is 44.28% of the total amount of people considered in the elderly group.\n",
        "* Interesting enough these are the groups that are most likely to be unemployed or working part-time, since the youngest group either don't have the experience to have a job or they are studying in a university so they don't have enough time to work in a full-time job.\n",
        "* In the elderly group side, this is the group that are most likely receiving their money from their pensions, meaning the elderly group is most likely unemployed or working part-time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LIaRM-23aDR"
      },
      "source": [
        "# Ok we have to create for each group risky and non-risky loans.\n",
        "df['Age_Group'] = np.nan\n",
        "\n",
        "lst = [df]\n",
        "\n",
        "for col in lst:\n",
        "    col.loc[(col['Age'] > 18) & (col['Age'] <= 29), 'Age_Group'] = 'Young'\n",
        "    col.loc[(col['Age'] > 29) & (col['Age'] <= 40), 'Age_Group'] = 'Young Adults'\n",
        "    col.loc[(col['Age'] > 40) & (col['Age'] <= 55), 'Age_Group'] = 'Senior'\n",
        "    col.loc[col['Age'] > 55, 'Age_Group'] = 'Elder' \n",
        "    \n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU8-7L_N4ZWq",
        "cellView": "code"
      },
      "source": [
        "# Lets find loans by age group and by the level of risk and plot them in a bar chart.\n",
        "\n",
        "# Age Group Segments\n",
        "young_good = df['Credit_amount'].loc[(df['Age_Group'] == 'Young') & (df['Risk'] == 'good')].sum()\n",
        "young_bad = df['Credit_amount'].loc[(df['Age_Group'] == 'Young') & (df['Risk'] == 'bad')].sum()\n",
        "young_adult_good = df['Credit_amount'].loc[(df['Age_Group'] == 'Young Adults') & (df['Risk'] == 'good')].sum()\n",
        "young_adult_bad = df['Credit_amount'].loc[(df['Age_Group'] == 'Young Adults') & (df['Risk'] == 'bad')].sum()\n",
        "senior_good = df['Credit_amount'].loc[(df['Age_Group'] == 'Senior') & (df['Risk'] == 'good')].sum()\n",
        "senior_bad = df['Credit_amount'].loc[(df['Age_Group'] == 'Senior') & (df['Risk'] == 'bad')].sum()\n",
        "elder_good = df['Credit_amount'].loc[(df['Age_Group'] == 'Elder') & (df['Risk'] == 'good')].sum()\n",
        "elder_bad = df['Credit_amount'].loc[(df['Age_Group'] == 'Elder') & (df['Risk'] == 'bad')].sum()\n",
        "\n",
        "# Percents\n",
        "young_good_p = young_good/(young_good + young_bad) * 100\n",
        "young_bad_p = young_bad/(young_good + young_bad) * 100\n",
        "young_adult_good_p = young_adult_good/(young_adult_good + young_adult_bad) * 100\n",
        "young_adult_bad_p = young_adult_bad/(young_adult_good + young_adult_bad) * 100\n",
        "senior_good_p = senior_good/(senior_good + senior_bad) * 100\n",
        "senior_bad_p =  senior_bad/(senior_good + senior_bad) * 100\n",
        "elder_good_p = elder_good/(elder_good + elder_bad) * 100\n",
        "elder_bad_p = elder_bad/(elder_good + elder_bad) * 100\n",
        "\n",
        "# Round Percents\n",
        "young_good_p = str(round(young_good_p, 2))\n",
        "young_bad_p = str(round(young_bad_p, 2))\n",
        "young_adult_good_p = str(round(young_adult_good_p, 2))\n",
        "young_adult_bad_p = str(round(young_adult_bad_p, 2))\n",
        "senior_good_p = str(round(senior_good_p, 2))\n",
        "senior_bad_p = str(round(senior_bad_p, 2))\n",
        "elder_good_p = str(round(elder_good_p, 2))\n",
        "elder_bad_p = str(round(elder_bad_p, 2))\n",
        "\n",
        "\n",
        "\n",
        "x = [\"Young\", \"Young Adults\", \"Senior\", \"Elder\"]\n",
        "\n",
        "good_loans = go.Bar(\n",
        "    x=x,\n",
        "    y=[young_good, young_adult_good, senior_good, elder_good],\n",
        "    name=\"Good Loans\",\n",
        "    text=[young_good_p + '%', young_adult_good_p + '%', senior_good_p + '%', elder_good_p + '%'],\n",
        "    textposition = 'auto',\n",
        "    marker=dict(\n",
        "        color='rgb(111, 235, 146)',\n",
        "        line=dict(\n",
        "            color='rgb(60, 199, 100)',\n",
        "            width=1.5),\n",
        "        ),\n",
        "    opacity=0.6\n",
        ")\n",
        "\n",
        "bad_loans =  go.Bar(\n",
        "    x=x,\n",
        "    y=[young_bad, young_adult_bad, senior_bad, elder_bad],\n",
        "    name=\"Bad Loans\",\n",
        "    text=[young_bad_p + '%', young_adult_bad_p + '%', senior_bad_p + '%', elder_bad_p + '%'],\n",
        "    textposition = 'auto',\n",
        "    marker=dict(\n",
        "        color='rgb(247, 98, 98)',\n",
        "        line=dict(\n",
        "            color='rgb(225, 56, 56)',\n",
        "            width=1.5),\n",
        "        ),\n",
        "    opacity=0.6\n",
        ")\n",
        "\n",
        "data = [good_loans, bad_loans]\n",
        "\n",
        "layout = dict(\n",
        "    title=\"Type of Loan by Age Group\", \n",
        "    xaxis = dict(title=\"Age Group\"),\n",
        "    yaxis= dict(title=\"Credit Amount\")\n",
        ")\n",
        "\n",
        "fig = go.Figure(data)\n",
        "\n",
        "HTML(fig.to_html())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grwlpx_e40vK"
      },
      "source": [
        "### Wealth Analysis:\n",
        "In this section we will analyse the amount of wealth our clients have by analyzing their checking accounts and whether the wealth status of our clients contribute to the risk of the loans Lending Club is issuing to customers.\n",
        "\n",
        "#### Summary:\n",
        "* Individuals belonging to the \"little wealth\" group, had a higher probability of being bad risk loans than other types fo groups.\n",
        "* The higher the wealth, the lower the probability of being a bad risk loan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yFd-c_c4gG_"
      },
      "source": [
        "# We have some missing value so we will just ignore the missing values in this analysis.\n",
        "df[\"Checking account\"].unique()\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idnUCIAY43GT"
      },
      "source": [
        "cross_checking = pd.crosstab(df['Risk'], df['Checking account']).apply(lambda x: x/x.sum() * 100)\n",
        "decimals = pd.Series([2,2,2], index=['little', 'moderate', 'rich'])\n",
        "\n",
        "cross_checking = cross_checking.round(decimals)\n",
        "cross_checking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch0D_pj24-CI"
      },
      "source": [
        "### High Risk Loans vs Low Risk Loans:\n",
        "In this section we will analyze both high and low risk loans. The most important thing is to find patters that could describe the some sort of correlation with these output values.\n",
        "\n",
        "#### Correlation (Our intent):\n",
        "In this part of the analysis, we want to look as to what feature affect directly the risk of the loan. In order to see these patterns, the first thing we have to do is to create a new column named \"Risk_int\" (Stands for risk in integer form) and involve this column in the correlation heatmap plot. \"0\" will stand for \"bad risk\" loans and \"1\" will stand for \"good risk\" loans.\n",
        "\n",
        "#### Summary:\n",
        "* The higher the credit amount borrowed, the most likely the loan will end up bad.\n",
        "* The higher the duration of the loan, the most likely the loan will turn out to be bad\n",
        "* Senior and Elders that asked for loans over 12k, have a high chance of becoming bad loans\n",
        "* If the credit amount borrowed is equivalent to 11,000 or more, the probability for the loan to be a bad one increases drastically. (Observe the Correlation of Risk with Credit Amount Borrowed.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTZnXJRG46-P"
      },
      "source": [
        "df['Risk_int'] = np.nan\n",
        "lst = [df]\n",
        "\n",
        "for col in lst:\n",
        "    col.loc[df['Risk'] == 'bad', 'Risk_int'] = 0 \n",
        "    col.loc[df['Risk'] == 'good', 'Risk_int'] = 1\n",
        "    \n",
        "    \n",
        "df['Risk_int'] = df['Risk_int'].astype(int)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqQO8IGo5AjI"
      },
      "source": [
        "# The higher the credit amount the higher the risk of the loan. Scatter plot?\n",
        "# The higher the duration of the loan the higher the risk of the loan?\n",
        "\n",
        "bad_credit_amount = df[\"Credit_amount\"].loc[df['Risk'] == 'bad'].values.tolist()\n",
        "good_credit_amount = df[\"Credit_amount\"].loc[df['Risk'] == 'good'].values.tolist()\n",
        "bad_duration = df['Duration'].loc[df['Risk'] == 'bad'].values.tolist()\n",
        "good_duration = df['Duration'].loc[df['Risk'] == 'good'].values.tolist()\n",
        "\n",
        "\n",
        "bad_loans = go.Scatter(\n",
        "    x = bad_duration,\n",
        "    y = bad_credit_amount,\n",
        "    name = 'Bad Loans',\n",
        "    mode = 'markers',\n",
        "    marker = dict(\n",
        "        size = 10,\n",
        "        color = 'rgba(152, 0, 0, .8)',\n",
        "        line = dict(\n",
        "            width = 2,\n",
        "            color = 'rgb(0, 0, 0)'\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "good_loans = go.Scatter(\n",
        "    x = good_duration,\n",
        "    y = good_credit_amount,\n",
        "    name = 'Good Loans',\n",
        "    mode = 'markers',\n",
        "    marker = dict(\n",
        "        size = 10,\n",
        "        color = 'rgba(34, 139, 34, .9)',\n",
        "        line = dict(\n",
        "            width = 2,\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "data = [bad_loans, good_loans]\n",
        "\n",
        "layout = dict(title = 'Correlation of Risk with <br> Credit Amount Borrowed',\n",
        "              yaxis = dict(zeroline = False),\n",
        "              xaxis = dict(zeroline = False)\n",
        "             )\n",
        "\n",
        "fig = go.Figure(data)\n",
        "\n",
        "HTML(fig.to_html())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilAVTywM5bbh"
      },
      "source": [
        "### Exploring Purposes of Loans:\n",
        "In this section my main aim is to see what purposes where most likely to bring most risk, in other words which of these pruposes were more likely to be considered high risk loans. Also, I would like to explore the operative side of the business, by determining which purposes where the ones that contributed the most towards loans issued.\n",
        "\n",
        "#### Summary:\n",
        "* Cars, Radio/TV and Furniture and Equipment made more than 50 % of the total risk and has the highest distribution of credit issued\n",
        "* The rest of the purposes were not frequent purposes in applying for a loan.\n",
        "* Cars and Radio/TV purposes were the less risky from the operative perspective since it had the widest gap between good and bad risk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtTP6AsO5I4f"
      },
      "source": [
        "df['Purpose'].unique()\n",
        "\n",
        "cross_purpose = pd.crosstab(df['Purpose'], df['Risk']).apply(lambda x: x/x.sum() * 100)\n",
        "cross_purpose = cross_purpose.round(decimals=2)\n",
        "cross_purpose.sort_values(by=['bad'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFWRPiHh5iof"
      },
      "source": [
        "### Predictive Modelling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRGj9zHQ5fO3"
      },
      "source": [
        "# Create inputs \n",
        "inputs = [vectice.create_dataset_version().with_parent_name('German-Credit-Data')]\n",
        "# Create a run\n",
        "run = vectice.create_run('Data Cleaning', JobType.PREPARATION)\n",
        "vectice.start_run(run, inputs = inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCURcvgSaWG0"
      },
      "source": [
        "### Data Cleaing \n",
        "In machine learning, if the data is irrelevant or error-prone then it leads to an incorrect model being built."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn40czAh5jOY"
      },
      "source": [
        "# Check missing values in our dataframe\n",
        "original_df.isnull().sum().sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIzHryHg5nyA"
      },
      "source": [
        "# We will drop the columns that have missing values although we will be loosing some information. Hopefully this does not cause\n",
        "# the model to underfit in the future.\n",
        "original_df.drop(['Checking account', 'Saving accounts'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqGHQabo5qTw"
      },
      "source": [
        "original_df.isnull().sum().sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgznLS335yaG"
      },
      "source": [
        "# In a real world scenario you could upload datasets into the GCS Bucket and update runs with the new dataset and add it through the Vectice App.\n",
        "# For example -> original_df.to_csv(GCS_URI)\n",
        "# Create outputs \n",
        "outputs = [vectice.create_dataset_version().with_parent_name(\"data cleaned\")]\n",
        "# End run\n",
        "vectice.end_run(outputs = outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzzyHOqUrSsw"
      },
      "source": [
        "### Train-Test Split Evaluation \n",
        "The procedure involves taking a dataset and dividing it into two subsets. The first subset is used to fit the model and is referred to as the training dataset. The second subset is not used to train the model; instead, the input element of the dataset is provided to the model, then predictions are made and compared to the expected values. This second dataset is referred to as the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U2pdsh250TS"
      },
      "source": [
        "# Feature Engineering (We cannot delete the missing values because we have too little data)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit\n",
        "\n",
        "original_df[\"Risk\"].value_counts() # 70% is good risk and 30% is bad risk.\n",
        "\n",
        "stratified = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
        "\n",
        "for train, test in stratified.split(original_df, original_df[\"Risk\"]):\n",
        "    strat_train = original_df.loc[train]\n",
        "    strat_test = original_df.loc[test]\n",
        "    \n",
        "\n",
        "# The main purpose of this code is to have an approximate ratio\n",
        "# of 70% good risk and 30% bad risk in both training and testing sets.\n",
        "strat_train[\"Risk\"].value_counts() / len(df) \n",
        "strat_test[\"Risk\"].value_counts() / len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQyjOGTUsgDo"
      },
      "source": [
        "### Imbalanced Classification Problem\n",
        "The number of examples that belong to each class may be referred to as the class distribution.\n",
        "\n",
        "Imbalanced classification refers to a classification predictive modeling problem where the number of examples in the training dataset for each class label is not balanced.\n",
        "\n",
        "That is, where the class distribution is not equal or close to equal, and is instead biased or skewed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKRELBBk4iQ3"
      },
      "source": [
        "# Create inputs \n",
        "inputs = [vectice.create_dataset_version().with_parent_name('German-Credit-Data')]\n",
        "# Create a run\n",
        "run = vectice.create_run('Train Test Split', JobType.PREPARATION)\n",
        "# Start the run\n",
        "vectice.start_run(run, inputs = inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqCY0s3Y52QJ"
      },
      "source": [
        "# Have our new train and test data\n",
        "train = strat_train\n",
        "test = strat_test\n",
        "\n",
        "\n",
        "# Our features\n",
        "X_train = train.drop('Risk', axis=1)\n",
        "X_test = test.drop('Risk', axis=1)\n",
        "\n",
        "# Our Labels we will use them later\n",
        "y_train = train[\"Risk\"]\n",
        "y_test = test[\"Risk\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hT-1ZF04iQ4"
      },
      "source": [
        "# In a real world scenario you could upload datasets into the GCS Bucket and update runs with the new dataset and add it through the Vectice App.\n",
        "# For example -> train.to_csv(GCS_URI) & test.to_csv(GCS_URI)\n",
        "# Create outputs, in the Vectice App you can add the train and test data together and name it \"train test data\"\n",
        "outputs = [vectice.create_dataset_version().with_parent_name(\"train test data\")]\n",
        "# End run\n",
        "vectice.end_run(outputs = outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mban2XtF54N0"
      },
      "source": [
        "# This is just a custom encoder that will be used in the Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.utils import check_array\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy import sparse\n",
        "\n",
        "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
        "                 handle_unknown='error'):\n",
        "        self.encoding = encoding\n",
        "        self.categories = categories\n",
        "        self.dtype = dtype\n",
        "        self.handle_unknown = handle_unknown\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
        "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
        "                        \"or 'ordinal', got %s\")\n",
        "            raise ValueError(template % self.handle_unknown)\n",
        "\n",
        "        if self.handle_unknown not in ['error', 'ignore']:\n",
        "            template = (\"handle_unknown should be either 'error' or \"\n",
        "                        \"'ignore', got %s\")\n",
        "            raise ValueError(template % self.handle_unknown)\n",
        "\n",
        "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
        "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
        "                             \" encoding='ordinal'\")\n",
        "\n",
        "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
        "\n",
        "        for i in range(n_features):\n",
        "            le = self._label_encoders_[i]\n",
        "            Xi = X[:, i]\n",
        "            if self.categories == 'auto':\n",
        "                le.fit(Xi)\n",
        "            else:\n",
        "                valid_mask = np.in1d(Xi, self.categories[i])\n",
        "                if not np.all(valid_mask):\n",
        "                    if self.handle_unknown == 'error':\n",
        "                        diff = np.unique(Xi[~valid_mask])\n",
        "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
        "                               \" during fit\".format(diff, i))\n",
        "                        raise ValueError(msg)\n",
        "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
        "\n",
        "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
        "        n_samples, n_features = X.shape\n",
        "        X_int = np.zeros_like(X, dtype=np.int)\n",
        "        X_mask = np.ones_like(X, dtype=np.bool)\n",
        "\n",
        "        for i in range(n_features):\n",
        "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
        "\n",
        "            if not np.all(valid_mask):\n",
        "                if self.handle_unknown == 'error':\n",
        "                    diff = np.unique(X[~valid_mask, i])\n",
        "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
        "                           \" during transform\".format(diff, i))\n",
        "                    raise ValueError(msg)\n",
        "                else:\n",
        "                    # Set the problematic rows to an acceptable value and\n",
        "                    # continue `The rows are marked `X_mask` and will be\n",
        "                    # removed later.\n",
        "                    X_mask[:, i] = valid_mask\n",
        "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
        "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
        "\n",
        "        if self.encoding == 'ordinal':\n",
        "            return X_int.astype(self.dtype, copy=False)\n",
        "\n",
        "        mask = X_mask.ravel()\n",
        "        n_values = [cats.shape[0] for cats in self.categories_]\n",
        "        n_values = np.array([0] + n_values)\n",
        "        indices = np.cumsum(n_values)\n",
        "\n",
        "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
        "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
        "                                n_features)[mask]\n",
        "        data = np.ones(n_samples * n_features)[mask]\n",
        "\n",
        "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
        "                                shape=(n_samples, indices[-1]),\n",
        "                                dtype=self.dtype).tocsr()\n",
        "        if self.encoding == 'onehot-dense':\n",
        "            return out.toarray()\n",
        "        else:\n",
        "            return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf48YRJW6Asn"
      },
      "source": [
        "# Scikit-Learn does not handle dataframes in pipeline so we will create our own class.\n",
        "# Reference: Hands-On Machine Learning\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "# Create a class to select numerical or cateogrical columns.\n",
        "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, attribute_names):\n",
        "        self.attribute_names = attribute_names\n",
        "    def fit (self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return X[self.attribute_names].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chOmt6Iz6Eng"
      },
      "source": [
        "# Create a run\n",
        "run = vectice.create_run(\"SVC\", job_type=JobType.TRAINING)\n",
        "# Create inputs \n",
        "inputs = [vectice.create_dataset_version().with_parent_name(\"train test data\")]\n",
        "# Start run \n",
        "vectice.start_run(run, inputs = inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGywe6KPrM9s"
      },
      "source": [
        "### Pipelines\n",
        "In most machine learning projects the data that you have to work with is unlikely to be in the ideal format for producing the best performing model. There are quite often a number of transformational steps such as encoding categorical variables, feature scaling and normalisation that need to be performed. Scikit-learn has built in functions for most of these commonly used transformations in it’s preprocessing package.\n",
        "However, in a typical machine learning workflow you will need to apply all these transformations at least twice. Once when training the model and again on any new data you want to predict on. Of course you could write a function to apply them and reuse that but you would still need to run this first and then call the model separately. Scikit-learn pipelines are a tool to simplify this process. They have several key benefits:\n",
        "* They make your workflow much easier to read and understand.\n",
        "* They enforce the implementation and order of steps in your project.\n",
        "* These in turn make your work much more reproducible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ5leWnc6GCS"
      },
      "source": [
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "numeric_train_df = X_train.select_dtypes(exclude=['object'])\n",
        "numeric_test_df = X_test.select_dtypes(exclude=['object'])\n",
        "\n",
        "categorical_train_df = X_train.select_dtypes(['object'])\n",
        "categorical_test_df = X_test.select_dtypes(['object'])\n",
        "\n",
        "numerical_pipeline = Pipeline([\n",
        "    (\"select_numeric\", DataFrameSelector(numeric_train_df.columns.values.tolist())),\n",
        "    (\"std_scaler\", StandardScaler()),\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('select_categoric', DataFrameSelector(categorical_train_df.columns.values.tolist())),\n",
        "    ('encoding', CategoricalEncoder(encoding='onehot-dense'))\n",
        "])\n",
        "\n",
        "# Combine both pipelines\n",
        "main_pipeline = FeatureUnion(transformer_list=[\n",
        "    ('num_pipeline', numerical_pipeline),\n",
        "    ('cat_pipeline', categorical_pipeline)\n",
        "])\n",
        "\n",
        "X_train_scaled = main_pipeline.fit_transform(X_train)\n",
        "X_test_scaled = main_pipeline.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY7_U2tb6KuQ"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encode = LabelEncoder()\n",
        "y_train_scaled = encode.fit_transform(y_train)\n",
        "y_test_scaled = encode.fit_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A6bsPGJmCXa"
      },
      "source": [
        "### Support Vector Machine: C-Support Vector Classification\n",
        "\n",
        "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
        "\n",
        "![Image](https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_svc_0011.png)\n",
        "\n",
        "[More info](https://scikit-learn.org/stable/modules/svm.html#svm-classification)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1SAFOto6Meo"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Implement gridsearchcv to see which are our best p\n",
        "\n",
        "params = {'C': [0.75, 0.85, 0.95, 1], \n",
        "          'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n",
        "          'degree': [3, 4, 5]}\n",
        "\n",
        "svc_clf = SVC(random_state=42)\n",
        "\n",
        "grid_search_cv = GridSearchCV(svc_clf, params)\n",
        "grid_search_cv.fit(X_train_scaled, y_train_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KixdE2-onYVU"
      },
      "source": [
        "### GridSearchCV: Hyper Parameter Tuning\n",
        "It's an exhaustive search over specified parameter values for an estimator.\n",
        "\n",
        "So it takes an estimator (eg. SVC) and uses a parameter list. Like below: Cross Validation is then performed in order to attain the best combinations of the parameters. Other methods like RandomSearchCV and Genetic Algorithms can be used aswell.\n",
        "\n",
        "```\n",
        "params = {'C': [0.75, 0.85, 0.95, 1], \n",
        "          'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n",
        "          'degree': [3, 4, 5]}\n",
        "```\n",
        "The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_oA8VMz6OfY"
      },
      "source": [
        "grid_search_cv.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g3VeNei6Qf3"
      },
      "source": [
        "grid_search_cv.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwmT5Q7X6SDJ"
      },
      "source": [
        "svc_clf = grid_search_cv.best_estimator_\n",
        "svc_clf.fit(X_train_scaled, y_train_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHxis5EX6VDZ"
      },
      "source": [
        "score = svc_clf.score(X_train_scaled, y_train_scaled)\n",
        "score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACIpB5CQ6WiI"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Let's make sure the data is not overfitting\n",
        "svc_clf = SVC(kernel='rbf', C=1, random_state=42)\n",
        "scores = cross_val_score(svc_clf, X_train_scaled, y_train_scaled)\n",
        "scores.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngas-svC6YJv"
      },
      "source": [
        "# Create model version\n",
        "model_version = [\n",
        "        vectice.create_model_version().with_parent_name('SVC').with_user_version(\"Run\").with_algorithm('Classification').with_properties([(x,str(y)) for x,y in grid_search_cv.best_params_.items()]).with_metrics([(\"Score\", score), (\"CV score\", scores.mean()).with_type(ModelType.CLASSIFICATION).with_status(ModelVersionStatus.EXPERIMENTATION)])\n",
        "    ]\n",
        "# End run \n",
        "vectice.end_run(outputs = model_version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j62R8ZTrrC7q"
      },
      "source": [
        "#### End!\n",
        "\n",
        "Congratulations and as Jake Peralta would say:\n",
        "\n",
        "![Image](https://i.imgur.com/I1wR7mE.gif?noredirect)"
      ]
    }
  ]
}